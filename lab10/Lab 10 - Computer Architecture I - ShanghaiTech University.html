<!DOCTYPE html>
<!-- saved from url=(0060)https://robotics.shanghaitech.edu.cn/courses/ca/22s/labs/10/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Lab 10 - Computer Architecture I - ShanghaiTech University</title>
<link rel="shortcut icon" type="image/png" href="https://robotics.shanghaitech.edu.cn/courses/ca/22s/favicon.ico">

<style type="text/css">
code {background-color: #ddeeff; color: #000000;}
kbd {background-color: #ddeeff; color: #000000;}
.new {background-color: #ffff00; color: #000000;}

div.checkoff {
  background:#eeeee0;
  padding:0.5em 1.5em 0.5em 1.5em;
  border-radius:1em;
  border:1px solid #ddd;
}

pre {
  background:#adc384;
  padding:0.5em 1.5em 0.5em 1.5em;
/*   border-radius:1em; */
  border:1px solid #ddd;
}


</style>
<link rel="stylesheet" type="text/css" href="./Lab 10 - Computer Architecture I - ShanghaiTech University_files/style.css">

</head>
<body>
<header>
<h2>Lab 10</h2>
</header>
<a href="https://toast-lab.sist.shanghaitech.edu.cn/courses/CS110@ShanghaiTech/Spring-2023/index.html">Computer Architecture I</a> <a href="http://www.shanghaitech.edu.cn/">ShanghaiTech University</a><br>

<h2 id="objectives">Objectives:</h2>
<ul>
  <li>Learn about basic OpenMP directives</li>
  <li>Write code to learn two ways of how <code>#pragma omp for</code> could be implemented. Learn about false sharing.</li>
  <li>Learn about basic multi-processing programming</li>
</ul>

<h2 id="setup">Setup</h2>
<p>
Download source code from <a href="./lab_10.zip">here</a>
</p>

<h2 id="multi-threading-programming-using-openmp">Multi-threading programming using OpenMP</h2>
<p>OpenMP stands for Open specification for Multi-Processing. It is a framework that offers a C interface. It is not a built-in part of the language â€“ most OpenMP features are directives to the compiler.</p>

<p>Benefits of multi-threaded programming using OpenMP include:</p>
<ul>
  <li>Very simple interface allows a programmer to separate a program into serial regions and parallel regions.</li>
  <li>Convenient synchronization control (Data race bugs in POSIX threads are very hard to trace).</li>
</ul>

<p>In this lab, we will practice on basic usage of OpenMP.</p>

<h3 id="exercise-1---openmp-hello-world">Exercise 1 - OpenMP Hello World</h3>
<p>Consider the implementation of Hello World (<tt>hello.c</tt>):</p>

<div><pre>int main ()
{
  #pragma omp parallel
  {
    int thread_ID = omp_get_thread_num ();
    printf (" hello world %d\n", thread_ID);
  }
  return 0;
}</pre></div>

<p>This program will fork off the default number of threads and each thread will print out "hello world" in addition to which thread number it is. You can change the number of OpenMP threads by setting the environment variable <tt>OMP_NUM_THREADS</tt> or by using the <a href="https://gcc.gnu.org/onlinedocs/libgomp/omp_005fset_005fnum_005fthreads.html"><tt>omp_set_num_threads</tt></a> function in your program. The <tt>#pragma</tt> tells the compiler that the rest of the line is a directive, and in this case it is omp parallel. <tt>omp</tt> declares that it is for OpenMP and <tt>parallel</tt> says the following code block (what is contained in { }) can be executed in parallel. Give it a try:
</p><div><pre>$ make hello &amp;&amp; ./hello</pre></div>

<p>If you run <tt>./hello</tt> a couple of times, you should see that the numbers are not always in numerical order and will most likely vary across runs. Think about the reason and explain to your TA.</p>

<p>It is also vital to note that the variable <tt>thread_ID</tt> is local to a specific thread and not shared across all threads. In general with OpenMP, variables declared inside the parallel block will be private to each thread, but variables declared outside will be global and accessible by all the threads.</p>

<h3 id="exercise-2---matrix-multiplication">Exercise 2 - Matrix Multiplication</h3>
<p>Matrix multiplication is a common operation in scientific computing and machine learning. In this exercise, we will optimize a matrix multiplication implementation using OpenMP. The matrix multiplication is implemented in <tt>matmul.c</tt>.</p>
<p>Your task is to optimize <tt>matmul.c</tt> (speedup may plateau as the number of threads continues to increase). To aid you in this process, two useful OpenMP functions are:</p>
<ul>
  <li><a href="https://gcc.gnu.org/onlinedocs/libgomp/omp_005fget_005fnum_005fthreads.html"><tt>int omp_get_num_threads()</tt></a></li>
  <li><a href="https://gcc.gnu.org/onlinedocs/libgomp/omp_005fget_005fthread_005fnum.html"><tt>int omp_get_thread_num()</tt></a></li>
</ul>
<p>Divide up the work for each thread through two different methods (write different code for each of these methods):</p>
<ol>
  <li>First task, <strong>slicing</strong>: have each thread handle adjacent rows: i.e. Thread 0 will compute the rows at indices <tt>i</tt> such that <tt>i % omp_get_num_threads()</tt> is <tt>0</tt>, Thread 1 will compute the rows where <tt>i % omp_get_num_threads()</tt> is <tt>1</tt>, etc.</li>
  <li>Second task, <strong>chunking</strong>: if there are N threads, break the matrices into N contiguous chunks along the first dimension (the rows), and have each thread compute the product of the chunk of matrix A and the entire matrix B.</li>
</ol>
<p>Hints:</p>
<ul>
  <li>Use the two functions we listed above somehow in the for loop to choose which rows each thread handles in the slicing method.</li>
  <li>You may need a special case to prevent going out of bounds for <tt>matmul_optimized_chunks</tt>. Don't be afraid to write one.</li>
  <li>Be careful about cache line alignment and false sharing. To avoid false sharing, each thread should have its own output buffer to store the computed rows.</li>
</ul>
<p>For this exercise, we are asking you to manually split the work amongst threads since this is a common pattern used in software optimization. The designers of OpenMP actually made the <tt>#pragma omp for</tt> directive to automatically split up independent work. Here is the function rewritten using it. <strong>You may NOT use this directive in your solution to this exercise</strong>.</p>
<div><pre>void matmul (double *a, double *b, double *c)
{
  #pragma omp parallel for 
  for (int i = 0; i &lt; MATRIX_SIZE; i++) {
    for (int j = 0; j &lt; MATRIX_SIZE; j++) {
      for (int k = 0; k &lt; MATRIX_SIZE; k++) {
        c[i * MATRIX_SIZE + j] += a[i * MATRIX_SIZE + k] * b[k * MATRIX_SIZE + j];
      }
    }
  }
}</pre></div>
<p>Test the performance of your code with:</p>
<div><pre>$ make matmul &amp;&amp; ./matmul</pre></div>
<div class="checkoff"><h4>Checkoff</h4>
<ul>
	<li>Show the modified code for both the slicing and chunking methods in <tt>matmul.c</tt>.</li>
	<li>Describe the performance differences between the methods you implemented and try to analyze the reason(Run more times to find a common pattern instead of just running once).</li>
	<li>Explain why using OpenMP may not necessarily lead to optimal performance on a single compute node with multiple cores.</li>
	<li>Bonus: Implement an additional optimization and discuss its impact on performance.</li>
</ul></div>

<h3 id="exercise-3---dot-product">Exercise 3 - Dot Product</h3>

<p>The next task is to compute the dot product of two vectors. At first glance, implementing this might seem not too different from <tt>v_add</tt>, but the challenge is how to sum up all of the products into the same variable (reduction). A sloppy handling of reduction may lead to <strong>data races</strong>: all the threads are trying to read and write to the same address simultaneously. One solution is to use a <strong>critical section</strong>. The code in a critical section can only be executed by a single thread at any given time. Thus, having a critical section naturally prevents multiple threads from reading and writing to the same data, a problem that would otherwise lead to data races. One way to avoid data races is to use the <tt>critical</tt> primitive provided by OpenMP. An implementation, <tt>dotp_naive</tt> in <tt>dotp.c</tt>, protects the sum with a critical section.</p>

<p>Try out the code (<tt>make dotp &amp;&amp;./dotp</tt>). Notice how the performance gets much worse as the number of threads goes up. By putting all of the work of reduction in a critical section, we have flattened the parallelism and made it so only one thread can do useful work at a time (not exactly the idea behind thread-level parallelism). This contention is problematic; each thread is constantly fighting for the critical section and only one is making any progress at any given time. As the number of threads goes up, so does the contention, and the performance pays the price. Can we reduce the number of times that each thread needs to use a critical section?</p>

<p>In this exercise, you have 2 tasks:</p>
<ol>
	<li>Fix the performance problem without using OpenMP's built-in Reduction keyword.</li>
	<li>Fix the performance problem using OpenMP's built-in Reduction keyword. (Note that your code should no longer contain <tt>#pragma omp critical</tt>)</li>
</ol>

<div class="checkoff"><h4>Checkoff</h4>
<ul>
	<li>Show the TA your manual fix to <tt>dotp.c</tt> that gets speedup over the single threaded case.</li>
	<li>Show the TA your Reduction keyword fix for <tt>dotp.c</tt>, and explain the difference in performance.</li>
</ul></div>

<footer>
    <hr style="clear: both;">
    <div style="float: left;">
        <address>
            Zongze Li &lt;<code>lizz</code> AT <code>shanghaitech.edu.cn</code>&gt;<br/>
        </address>
        <br/>
        Last modified:
        <time datetime="2023-04-26">2023-04-26</time>
    </div>
</footer>

</body></html>